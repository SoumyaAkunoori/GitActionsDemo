name: CI

on:
  push:
    branches:
      - main
    #paths:
    #  - databricks/databricks/**/*.py
jobs:

  pylint:
    name: pylint report
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v3.4.0
      
    - name: Install Pylint
      run: pip install pylint

    - name: Run PyLint on Notebooks
      run: find databricks/ -name "*.py" -exec pylint --output-format=json {} \; > pylint_report.json
 
    - name: Upload PyLint Report
      uses: actions/upload-artifact@v3.1.2
      with:
        name: pylint-report
        path: pylint_report.json
    # Check Markdown
    - name: markdownlint-cli2-action
      uses: DavidAnson/markdownlint-cli2-action@v9.0.0

  pytest:
    name: pytest and coverage report
    runs-on: ubuntu-latest
    needs: pylint
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v3.4.0
      
    - name: Install Dependencies
      run: pip install -r requirements.txt
      
    - name: Run Pytest and Coverage
      run: |
        find databricks/ -type f -name "*.py" -exec sh -c 'pytest --cov=databricks --cov-report=xml --junitxml=pytest_report.xml "$@"' _ {} +
        coverage xml
      continue-on-error: true

    
    - name: Upload Pytest and Coverage Reports
      uses: actions/upload-artifact@v2
      with:
        name: pytest-and-coverage-reports
        path: |
          pytest_report.xml
          coverage.xml

  sonarqube:
    name: Sonarqube scan
    runs-on: ubuntu-latest
    needs: pytest
    steps:
      - name: Check if secret SONAR_TOKEN is available
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        run: |
          if [[ ! ${SONAR_TOKEN} ]]; then
            echo "::error::Secret \`SONAR_TOKEN\` was not found: it is required to use SonarQube."
            exit 1
          fi
      - uses: actions/checkout@v3.4.0
        with:
          fetch-depth: 0  # Shallow clones should be disabled for a better relevancy of analysis
      - uses: sonarsource/sonarqube-scan-action@master
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
      # If you wish to fail your job when the Quality Gate is red, uncomment the
      # following lines. This would typically be used to fail a deployment.
      # - uses: sonarsource/sonarqube-quality-gate-action@master
      #   timeout-minutes: 5
      #   env:
      #     SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}

 name: Deploy Notebooks to Azure Databricks

on:
  push:
    branches:
      - main

env:
  AZURE_SUBSCRIPTION_ID: 7b206cd2-248a-49a7-b5e0-6adf9df31cb2
  AZURE_RESOURCE_GROUP: Test-rg
  AZURE_DATABRICKS_WORKSPACE: ADB-GIT-Demo
  AZURE_DATABRICKS_TOKEN: dapi61f701468bffc505edc50e9af2ace6a9-3
  AZURE_DATABRICKS_CLUSTER_ID: 0320-073504-caw7ti2 
  NOTEBOOKS_FOLDER: databricks/databricks

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Install Databricks CLI
        run: |
          python3 -m pip install databricks-cli

      - name: Login to Azure
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Deploy Notebooks
        run: |
          # Authenticate to the Databricks workspace
          databricks configure --token dapi61f701468bffc505edc50e9af2ace6a9-3 --url https://adb-1128000650204589.9.azuredatabricks.net

          # Get the ID of the Databricks workspace
          workspace_id=$(databricks workspace list --output JSON | jq --raw-output '.[] | select(.path == "/") | .id')

          # Get the ID of the Databricks cluster
          cluster_id=$(databricks clusters get --cluster-id "0320-073504-caw7ti2" --output JSON | jq --raw-output '.cluster_id')

          # Get the ID of the destination folder
          destination_folder_id=$(databricks workspace mkdirs --path "/databricks/databricks" --output JSON | jq --raw-output '.path')

          # Deploy notebooks to the destination folder
          for notebook in $(find . -name '*.py'); do
            notebook_name=$(basename "$databricks")
            databricks workspace import -l PYTHON -f OVERWRITE --cluster-id "0320-073504-caw7ti2" --path "/databricks/databricks/${databricks}" "${io_utils.py}"
          done

    
  

